docker exec -u 0 -it jenkins-master sh -c "chmod 666 /var/run/docker.sock"

Phase 1: Infrastructure Provisioning & Security Configuration
1.1 Overview
The primary objective of Phase 1 was to establish a secure, reproducible infrastructure foundation for the DevSecOps pipeline. Instead of manually creating servers in the AWS Console, we utilized Terraform (Infrastructure as Code) to define our resources. This ensures that the environment can be destroyed and recreated instantly with zero configuration drift.

A critical design decision in this phase was to adopt a "Security First" network architecture. We deliberately restricted the AWS Security Group to block all incoming traffic on monitoring ports (9090 for Prometheus, 3000 for Grafana, 5000 for the App), allowing only standard SSH (22) and HTTP (80) traffic. This forced us to implement secure tunneling methods to access our internal tools.

1.2 Infrastructure Automation with Terraform
We defined our infrastructure in a main.tf file, specifying an AWS EC2 instance (t2.micro), a generated SSH Key Pair, and a strict Security Group.

Key Configuration: The Security Group
To simulate a real-world enterprise environment, we applied strict firewall rules:

Inbound Rule 1: Allow Port 22 (SSH) – Restricted to admin access.

Inbound Rule 2: Allow Port 80 (HTTP) – Public web traffic.

Blocked Ports: 3000 (Grafana), 9090 (Prometheus), 5000 (Flask App).

Reasoning: Monitoring tools expose sensitive metric data and should never be exposed directly to the public internet.

Execution Commands
The following Terraform lifecycle commands were used to provision the environment:

Bash

# 1. Initialize the Terraform backend and download AWS providers
terraform init

# 2. Generate an execution plan to preview changes (Sanity Check)
terraform plan

# 3. Apply the infrastructure changes (Auto-approve to bypass manual confirmation)
terraform apply --auto-approve
Outcome: Terraform provisioned the EC2 instance and downloaded the private key (aws_key) locally for access.

1.3 Server Configuration & Dependency Installation
Once the server was up, we needed to configure the runtime environment. This involved updating the package repositories and installing the core tools required for our CI/CD pipeline: Git, Docker, and Jenkins.

Accessing the Server
We logged in using the private key generated by Terraform:

Bash

ssh -i "aws_key" ec2-user@<AWS_PUBLIC_IP>
Installation Commands
We used the standard yum package manager for Amazon Linux 2:

Bash

# Update the OS packages
sudo yum update -y

# Install Git (for source control) and Docker (for containerization)
sudo yum install git docker -y

# Start the Docker service
sudo service docker start
1.4 Solving the "Permission Denied" Challenge
The Problem: Upon installing Docker and Jenkins, we encountered a critical error when the pipeline tried to build an image. Jenkins failed with the message:

dial unix /var/run/docker.sock: connect: permission denied

Root Cause: By default, the Docker daemon binds to a Unix socket that is owned by root. The ec2-user and the jenkins user do not have permission to access this socket, meaning they cannot run any docker commands.

The Solution: We had to modify the user groups and file permissions to grant access to the Docker socket.

Fix Commands:
Bash

# 1. Add the current user (ec2-user) to the 'docker' group
# This allows running docker commands without 'sudo'
sudo usermod -aG docker ec2-user

# 2. (CRITICAL FIX) Grant Read/Write permissions to the Docker Socket
# This allows the Jenkins user (which runs the pipeline) to communicate
# with the Docker daemon to spawn sibling containers.
sudo chmod 666 /var/run/docker.sock
1.5 Accessing Blocked Ports: SSH Tunneling
The Problem: Because we strictly blocked Ports 9090 (Prometheus), 3000 (Grafana), and 5000 (Flask API) in our Terraform Security Group, we could not access our dashboards via the browser (e.g., http://47.128.x.x:3000 failed with "Connection Refused").

The Solution: SSH Local Port Forwarding Instead of weakening our firewall by opening these ports to the world, we established an SSH Tunnel. This technique forwards traffic from a specific port on the local machine (laptop) through the encrypted SSH connection to the remote server (AWS).

This effectively tricks the local browser into thinking the remote tools are running locally.

The Tunneling Command:
We ran this command from a local PowerShell/Terminal window (kept running in the background):

PowerShell

# Syntax Breakdown:
# -i "key": Use the Terraform-generated key
# -L [LocalPort]:127.0.0.1:[RemotePort]: Map local port to remote localhost
# ec2-user@IP: The remote server address

ssh -i "C:\Users\Ashish\.ssh\aws_key" \
    -L 9090:127.0.0.1:9090 \
    -L 3000:127.0.0.1:3000 \
    -L 5001:127.0.0.1:5000 \
    ec2-user@47.128.226.87
Result:

Prometheus: Accessible via http://localhost:9090

Grafana: Accessible via http://localhost:3000

App Health Check: Accessible via http://localhost:5001/health

This setup allowed us to maintain a "Zero Trust" network posture on AWS while retaining full observability access for the development team.


# Phase 2: CI/CD Pipeline Configuration & Container Orchestration

## **2.1 Overview**

With the secure infrastructure and permissions established in Phase 1, the objective of Phase 2 was to implement a fully automated **Continuous Integration and Continuous Deployment (CI/CD)** pipeline.

The goal was to move from manual execution to an automated workflow where every code commit triggers a build, test, and deployment sequence. We utilized **Jenkins** as the orchestration engine and **Docker Compose** to manage the multi-container application (Python Flask App + PostgreSQL Database).

A key challenge in this phase was establishing the "handshake" between Jenkins and the Docker Daemon, ensuring that the automation server could spawn and manage sibling containers without privilege escalation issues.

-----

## **2.2 Jenkins Initialization & Security Setup**

Although Jenkins was installed in Phase 1, it remained in a "locked" state. To begin building pipelines, we needed to unlock the administrative console and configure the initial security parameters.

### **Retrieving the Initial Admin Credential**

Jenkins generates a temporary cryptographic password upon first launch to prevent unauthorized access during setup. Since our server has no GUI, we had to retrieve this secret directly from the file system via the terminal.

**Command Executed:**

```bash
# Retrieve the initial administrator password
# This password is required to unlock the Jenkins UI at http://localhost:8080
sudo cat /var/lib/jenkins/secrets/initialAdminPassword
```

**Configuration Steps Taken:**

1.  **Unlock:** Pasted the retrieved password into the Jenkins portal (accessed via SSH Tunnel on `localhost:8080`).
2.  **Plugins:** Installed the "Suggested Plugins" pack (includes Git, Pipeline, and Workspace Cleanup).
3.  **Admin User:** Created the primary admin user (`admin`) to replace the temporary credential.

-----

## **2.3 Container Orchestration (Docker Compose)**

Instead of running disjointed `docker run` commands, we adopted an **Orchestration Strategy** using `docker-compose.yml`. This file serves as the "Blueprint" for our application, defining networking, ports, and environment variables in code.

### **The Architecture Definition**

We defined two primary services in our composition:

1.  **`app` (The Todo App):**
      * **Build Context:** Built from the local `Dockerfile`.
      * **Port Mapping:** `5000:5000` (Maps host port 5000 to container port 5000).
      * **Dependency:** Depends on the database service.
2.  **`db` (PostgreSQL):**
      * **Image:** `postgres:13-alpine` (Lightweight production database).
      * **Volume:** Persists data so tasks aren't lost when containers restart.

### **Why Docker Compose?**

Using Docker Compose solved the **"Dependency Hell"** problem. It ensures that the Database is always started *before* the Application, and it automatically creates a shared network (defaulting to `ec2-user_default`) so the containers can communicate using their service names.

-----

## **2.4 The Pipeline Execution Strategy**

We defined the automation logic in a `Jenkinsfile` using Groovy script. This pipeline creates an immutable build artifact and deploys it consistently.

### **Pipeline Stages Implemented:**

1.  **Checkout:** Pulls the latest code from the GitHub repository.
2.  **Build:** executes `docker-compose build`. This compiles the Python environment and installs `requirements.txt`.
3.  **Test:** Runs unit tests to ensure code quality before deployment.
4.  **Deploy:** Executes `docker-compose up -d`. This performs a "Rolling Update," replacing old containers with new ones with zero downtime.

### **The "Sibling Container" Mechanics**

Because we fixed the socket permissions in Phase 1 (`chmod 666 /var/run/docker.sock`), Jenkins was able to execute the following commands *inside* the pipeline without `sudo`:

```bash
# Jenkins executes this to start the application stack
docker-compose up -d --build
```

-----

## **2.5 Verification & Troubleshooting**

Once the pipeline successfully ran (indicated by Green Stages in the Jenkins View), we needed to verify that the containers were actually running and listening on the correct ports.

### **Status Check Command**

We used the process status command to inspect the runtime state of the stack.

**Command Executed:**

```bash
# List all running containers to verify Up/Down status and Port Mappings
docker ps
```

**Expected Output Analysis:**

  * **`todo-app`**: Status `Up` | Ports `0.0.0.0:5000->5000/tcp`
  * **`ec2-user-db-1`**: Status `Up` | Ports `5432/tcp`

### **The "Connection Refused" Check**

Initially, we verified the application locally on the server to ensure the internal firewall wasn't blocking localhost traffic.

**Command Executed:**

```bash
# Curl the health endpoint from INSIDE the server
curl http://localhost:5000/health
```

*Result:* `{"status": "up"}` – This confirmed the application logic was sound, and any connectivity issues were likely network/firewall related (which leads into Phase 3).

-----

## **2.6 Summary of Phase 2 Achievements**

In this phase, we successfully transformed a static server into a dynamic build factory.

  * **Automation:** We eliminated manual deployments.
  * **Consistency:** Docker Compose ensures the Dev environment matches Production.
  * **Security:** Jenkins runs as a non-root user, interacting with Docker via the hardened socket permissions established previously.

This laid the groundwork for **Phase 3**, where we would tackle the challenge of monitoring these containers across a restricted network.

-----


# Phase 3: Secure Remote Access & Network Tunneling

## **3.1 Overview**

With the infrastructure provisioning (Phase 1) and application deployment (Phase 2) complete, our application was running successfully on the AWS server. However, a major operational challenge remained: **Observability.**

We had installed Prometheus (Port 9090) and Grafana (Port 3000) inside the server, but we could not see them.

**The Architectural Dilemma:**

  * **Option A (Insecure):** Open Ports 9090 and 3000 in the AWS Security Group to `0.0.0.0/0`. This allows anyone on the internet to view our server metrics, potential vulnerabilities, and sensitive dashboard data.
  * **Option B (Secure):** Keep all ports blocked and use an encrypted tunnel to access the tools as if we were sitting physically next to the server.

We chose **Option B**. This phase details the implementation of **SSH Local Port Forwarding** to establish a secure management lane into our private network.

-----

## **3.2 The Security Constraint (The "Black Box" Problem)**

In Phase 1, we deliberately hardened our `main.tf` Terraform configuration.

  * **Allowed Inbound:** Port 22 (SSH) and Port 80 (HTTP).
  * **Blocked Inbound:** All other ports.

Because of this, trying to access the monitoring dashboards via the browser (e.g., `http://47.128.226.87:3000`) resulted in a **"Connection Refused"** or **"Site Can't Be Reached"** error. The AWS Firewall was doing exactly what it was designed to do: dropping unauthorized packets.

This created a "Black Box" scenario where the tools were running, but we had no visibility into them.

-----

## **3.3 The Solution: SSH Local Port Forwarding**

To solve this, we utilized **SSH Tunneling** (specifically Local Port Forwarding).

**How It Works:**
SSH Tunneling allows us to "wrap" arbitrary TCP traffic (like a web request to Grafana) inside the encrypted SSH protocol (Port 22).

1.  **The Entry:** We open a listening port on our **Laptop** (e.g., `localhost:3000`).
2.  **The Transport:** When we send a request to this local port, SSH captures the data, encrypts it, and pushes it through the existing SSH connection to the AWS server.
3.  **The Exit:** The AWS server decrypts the data and forwards it to the target port on its own `localhost` (Port 3000).

This effectively extends the server's internal network to our local machine securely.

-----

## **3.4 Implementation & Command Execution**

We executed the tunneling command from a local terminal (PowerShell or Bash). This command had to remain running in the background to keep the "pipe" open.

### **The Command:**

```powershell
# Establish the Encrypted Tunnel
ssh -i "C:\Users\Ashish\.ssh\aws_key" \
    -L 9090:127.0.0.1:9090 \
    -L 3000:127.0.0.1:3000 \
    -L 5001:127.0.0.1:5000 \
    ec2-user@47.128.226.87
```

### **Command Breakdown:**

  * **`-i "...\aws_key"`**: Identity file. Uses the private key generated by Terraform for authentication.
  * **`-L [LocalPort]:[RemoteHost]:[RemotePort]`**: The core tunneling flag.
      * **`-L 9090:127.0.0.1:9090`**: "Take traffic from my laptop's port 9090 and forward it to the server's port 9090." (For Prometheus).
      * **`-L 3000:127.0.0.1:3000`**: Same logic for Grafana.
      * **`-L 5001:127.0.0.1:5000`**: "Forward my laptop's port 5001 to the server's port 5000."
          * *Note:* We used local port `5001` here to avoid conflicts if `5000` was already taken on the laptop, demonstrating port mapping flexibility.
  * **`ec2-user@...`**: The target Bastion host (our EC2 instance).

-----

## **3.5 Access Verification**

Once the tunnel was established (verified by the blinking cursor in the terminal), we were able to access the internal tools using `localhost` URLs.

**1. Prometheus (The Data Engine):**

  * **URL:** `http://localhost:9090`
  * **Status:** Accessible.
  * *Observation:* We could now query the raw time-series database.

**2. Grafana (The Visualization Dashboard):**

  * **URL:** `http://localhost:3000`
  * **Status:** Accessible.
  * *Observation:* We could log in (default `admin/admin`), add data sources, and build charts.

**3. Application API (Health Check):**

  * **URL:** `http://localhost:5001/health`
  * **Status:** Accessible.
  * *Observation:* Returned `{"status": "up"}`, proving the app was live.

-----

## **3.6 Common Issues & Troubleshooting**

During this phase, we encountered specific behaviors inherent to tunneling:

**Problem 1: "Channel Open: Connect Failed"**

  * *Symptom:* The terminal shows connection errors even though SSH is connected.
  * *Cause:* The application on the remote end (e.g., the Grafana container) had crashed or was restarting.
  * *Fix:* Checked `docker ps` on the server to ensure containers were `Up`.

**Problem 2: "Bind: Address Already in Use"**

  * *Symptom:* The SSH command fails to start on the laptop.
  * *Cause:* We tried to open a new tunnel while an old one was still running (a "Stale Process").
  * *Fix:* Used `Ctrl+C` to kill the old session or manually found and killed the SSH process before restarting.

-----

## **3.7 Phase 3 Summary**

Phase 3 was the bridge between "Deployment" and "Observability."

  * **Security Posture:** We maintained a "Zero Trust" approach by keeping firewall ports closed.
  * **Accessibility:** We successfully established a secure management plane.
  * **Result:** We gained full visibility into the system without compromising its security.

This set the stage for **Phase 4**, where we would configure the internal communication between these now-accessible tools.

-----

# Phase 4: Monitoring Architecture & Internal Service Discovery

## **4.1 Overview**

With the secure tunnel established in Phase 3, we gained access to the Prometheus and Grafana dashboards. However, we immediately encountered a critical operational failure: **Prometheus Targets were DOWN (Red).**

Prometheus could not scrape metrics from our Application or Node Exporter. The error message was explicit:

> `Get "http://app:5000/metrics": dial tcp: lookup app on 127.0.0.11:53: no such host`

**The Goal:**
The objective of Phase 4 was to debug the Docker internal network, resolve DNS resolution failures, and force the isolated containers to communicate over a shared internal network.

-----

## **4.2 The "Split Network" Challenge**

**The Problem:**
When Jenkins deployed the application using `docker-compose`, it automatically created a new, isolated network (e.g., `ec2-user_default` or `project_network`) for the App and Database.
However, our Monitoring Stack (Prometheus/Grafana) was running on the default Docker `bridge` network because they were started manually or separately.

**The Consequence:**
Docker containers on different networks are strictly isolated. Prometheus (on Network A) was shouting out to the App (on Network B), but the firewall between them dropped the packets. They were in "different rooms" and could not see each other.

-----

## **4.3 The "Name Game" (DNS Resolution Failure)**

**The Problem:**
Our `prometheus.yml` configuration file was originally written to look for a host named **`app`**:

```yaml
- targets: ['app:5000']
```

However, inspecting the running containers revealed a mismatch. Jenkins and Docker Compose had dynamically named the container **`todo-app`** (or `ec2-user-app-1` depending on the build).

**The Diagnosis:**
Prometheus was trying to resolve the DNS name `app`, but the Docker internal DNS server only had a record for `todo-app`. This resulted in the `no such host` error.

-----

## **4.4 The Solution Part 1: Network Unification**

To allow the containers to talk, we had to manually attach the monitoring tools to the application's network.

**1. Discovery (Finding the Network Name):**
We first needed to identify exactly which network the new App container was using. We used the `docker inspect` command with a Go template to extract just the network name.

```bash
# Retrieve the network name of the running application
NETWORK_NAME=$(docker inspect todo-app -f '{{range $k, $v := .NetworkSettings.Networks}}{{$k}}{{end}}')

# Verify the output (e.g., 'ec2-user_default')
echo $NETWORK_NAME
```

**2. Bridging the Connection:**
Once we had the network name, we forced the monitoring containers to join that network. This is like plugging an ethernet cable from Prometheus into the App's switch.

```bash
# Connect Prometheus to the App's network
docker network connect $NETWORK_NAME ec2-user-prometheus-1

# Connect Grafana to the App's network (so it can query Prometheus)
docker network connect $NETWORK_NAME ec2-user-grafana-1

# Connect Node Exporter (so Prometheus can scrape CPU stats)
docker network connect $NETWORK_NAME node-exporter
```

-----

## **4.5 The Solution Part 2: Configuration Override**

Even with the networks bridged, Prometheus was still looking for the wrong name (`app`). We had to update the configuration on the server to match reality (`todo-app`).

**Command Executed:**
We overwrote the `prometheus.yml` file directly on the production server using a `cat` EOF block. This ensured the config matched the exact container names listed in `docker ps`.

```bash
cat <<EOF > /home/ec2-user/prometheus.yml
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'todo_app'
    static_configs:
      - targets: ['todo-app:5000']  # <--- FIXED: Updated to real container name

  - job_name: 'node_exporter'
    static_configs:
      - targets: ['node-exporter:9100']
EOF
```

**Applying the Fix:**
For Prometheus to read this new file, we had to restart the container.

```bash
docker restart ec2-user-prometheus-1
```

-----

## **4.6 Verification & Success**

After correcting the network path and the DNS names, we verified the fix:

**1. Prometheus Targets Check:**

  * Accessed `http://localhost:9090/targets` via the tunnel.
  * **Result:** The state for `todo_app` and `node_exporter` turned **GREEN (UP)**.
  * *Meaning:* Prometheus successfully reached out across the Docker network and scraped the metrics endpoint.

**2. Grafana Data Source Check:**

  * Accessed Grafana Settings -\> Data Sources.
  * Updated the URL to the internal container name: `http://ec2-user-prometheus-1:9090`.
  * **Result:** "Data source is working."

## **4.7 Summary of Phase 4**

This phase was the turning point of the project. We successfully:

1.  **Diagnosed** complex Docker networking isolation.
2.  **Identified** DNS naming mismatches between config and runtime.
3.  **Remediated** the issues by dynamically attaching networks and updating configurations on the fly.

This creates a robust internal mesh where our Monitoring tools can observe the Application without either of them needing to be exposed to the public internet.

-----
Here is the final part of your documentation. This covers **Phase 5 (Visualization & Metrics)**, **Phase 6 (Load Testing)**, and the **Project Conclusion**.

It includes the specific PromQL queries, the "Smart Quote" syntax errors we fixed, the "Builder vs. Code" confusion, and the traffic generation logic.

You can copy and paste this directly to complete your `README.md`.

-----

# Phase 5: Dashboard Engineering & Custom Metrics

## **5.1 Overview**

With the internal network bridged (Phase 4), Prometheus was successfully scraping data. The next objective was to visualize this data in **Grafana** to create an "Executive View" of the system's health.

This phase focused on translating raw time-series data into meaningful insights (Requests Per Second, Error Rates, and CPU Usage). We faced significant challenges with Grafana's query editor UI and PromQL syntax.

-----

## **5.2 Data Source Configuration**

Before building charts, Grafana needed to know where the data lived.

  * **Challenge:** The default URL `http://localhost:9090` failed because "localhost" inside the Grafana container refers to itself, not the Prometheus container.
  * **Fix:** We configured the Prometheus Data Source using the internal Docker DNS name established in Phase 4.
      * **URL:** `http://ec2-user-prometheus-1:9090`
      * **Access:** Server (Default)

-----

## **5.3 Implementing Custom Panels (PromQL)**

We designed three specific panels to monitor the "Golden Signals" of the application.

### **Panel 1: Total Traffic (Requests Per Second)**

  * **Goal:** Measure the volume of traffic hitting the application.
  * **Metric:** `flask_http_request_total`
  * **The Query:**
    ```promql
    sum(rate(flask_http_request_total[1m]))
    ```
  * **Explanation:** Calculates the per-second rate of requests, averaged over a 1-minute window, and sums them across all instances.

### **Panel 2: Error Rate % (Reliability)**

  * **Goal:** visualize what percentage of requests are failing (HTTP 500 status).
  * **The Query:**
    ```promql
    sum(rate(flask_http_request_total{status=~"5.."}[1m])) / sum(rate(flask_http_request_total[1m])) * 100
    ```
  * **Explanation:** (Rate of Errors / Total Rate of Traffic) \* 100. This gives a clear percentage line (e.g., "5% Error Rate").

### **Panel 3: CPU Usage % (Resource Monitoring)**

  * **Goal:** Monitor server load.
  * **The Query:**
    ```promql
    100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[1m])) * 100)
    ```
  * **Explanation:** Prometheus tracks how much time the CPU is *idle*. We calculate usage by inverting this: `100% - Idle% = Usage%`.

-----

## **5.4 Challenges & Troubleshooting**

**Issue 1: The "Smart Quote" Syntax Error**

  * **Symptom:** Grafana threw parsing errors: `unexpected character inside braces`.
  * **Root Cause:** Copy-pasting queries from chat tools or documents often converts straight quotes (`"`) into curly "smart quotes" (`”`). PromQL strictly requires straight quotes.
  * **Fix:** Manually sanitized all queries to use standard ASCII quotes.

**Issue 2: "Builder" vs. "Code" View**

  * **Symptom:** Pasting a complex query resulted in "No Data" or weird formatting.
  * **Root Cause:** Grafana defaults to a "Builder" UI (drag-and-drop), which confuses raw PromQL code.
  * **Fix:** We identified the "Code" toggle button in the top-right corner of the query editor, allowing us to paste raw PromQL directly.

**Issue 3: "No Data" (Static System)**

  * **Symptom:** The queries were correct, but the graphs were empty.
  * **Root Cause:** The `rate()` function requires *change* over time. Since the app was idle, the rate was effectively zero/null.
  * **Fix:** Implemented Load Testing (Phase 6).

-----

# Phase 6: Load Testing & System Validation

## **6.1 Overview**

A monitoring system is useless if it has nothing to monitor. To validate our dashboard, we needed to simulate a production environment with active users. We utilized a Python-based traffic generator to create "synthetic load."

## **6.2 Traffic Simulation**

We ran a `traffic_generator.py` script from our local machine (laptop). Because we had established the SSH Tunnel in Phase 3, we could point this script at `localhost:5001`, and the traffic was forwarded securely to the AWS application.

**The Script Logic:**

  * **Loop:** Infinite `while True` loop.
  * **Action:** Sent random HTTP GET requests to `/` (Success) and specific endpoints designed to trigger failures.
  * **Concurrency:** Generated consistent "noise" to wake up the Prometheus scrapers.

## **6.3 Visual Verification**

Once the script ran for \~2-3 minutes, we observed the "Heartbeat" of the system in Grafana:

1.  **RPS Panel:** Spiked from 0 to \~5-10 RPS.
2.  **Error Rate:** Showed intermittent spikes as the script hit failure endpoints.
3.  **CPU Usage:** Increased slightly due to request processing and monitoring overhead.

This confirmed that the entire pipeline—from the User -\> Tunnel -\> App -\> Exporter -\> Prometheus -\> Grafana—was fully operational.

-----

# Phase 7: Infrastructure Teardown (Cost Management)

## **7.1 The "Kill Switch"**

Since this project utilized paid AWS resources (EC2 instances, EBS volumes), it was critical to shut down the environment to avoid unnecessary billing.

Instead of manually terminating instances in the AWS Console (which often leaves behind "Ghost Resources" like Security Groups or Key Pairs), we used Terraform to perform a clean sweep.

## **7.2 Execution Command**

```bash
cd terraform
terraform destroy --auto-approve
```

**Outcome:**

  * Terminated the EC2 Instance.
  * Deleted the Security Group.
  * Removed the SSH Key Pair.
  * *Result:* Zero active costs.

-----

# Project Conclusion

## **Summary of Achievements**

This project successfully demonstrated an **End-to-End DevSecOps Pipeline**. We started with a blank AWS account and built a secure, monitored application platform using code-first principles.

**Key Technical Wins:**

  * **Security:** Implemented a "Zero Trust" network architecture with strict firewalls and SSH Bastion tunneling.
  * **Automation:** Replaced manual server setup with Terraform (IaC) and manual deployments with Jenkins (CI/CD).
  * **Observability:** Solved complex container networking isolation issues ("The Name Game") to establish a robust monitoring mesh using Prometheus and Grafana.

## **Future Scope**

To take this project from "MVP" to "Enterprise Grade," the following enhancements are recommended:

1.  **Alerting:** Configure Grafana Alert Manager to send Slack notifications when Error Rate \> 1%.
2.  **Orchestration:** Migrate from Docker Compose to **Kubernetes (EKS)** for auto-scaling and self-healing capabilities.
3.  **SSL/TLS:** Implement Nginx as a reverse proxy to serve the application over HTTPS instead of HTTP.

-----